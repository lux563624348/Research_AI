{
  "2502.06816v2": {
    "title": "DeepCell: Self-Supervised Multiview Fusion for Circuit Representation Learning",
    "authors": [
      "Zhengyuan Shi",
      "Chengyu Ma",
      "Ziyang Zheng",
      "Lingfeng Zhou",
      "Hongyang Pan",
      "Wentao Jiang",
      "Fan Yang",
      "Xiaoyan Yang",
      "Zhufei Chu",
      "Qiang Xu"
    ],
    "summary": "We introduce DeepCell, a novel circuit representation learning framework that\neffectively integrates multiview information from both And-Inverter Graphs\n(AIGs) and Post-Mapping (PM) netlists. At its core, DeepCell employs a\nself-supervised Mask Circuit Modeling (MCM) strategy, inspired by masked\nlanguage modeling, to fuse complementary circuit representations from different\ndesign stages into unified and rich embeddings. To our knowledge, DeepCell is\nthe first framework explicitly designed for PM netlist representation learning,\nsetting new benchmarks in both predictive accuracy and reconstruction quality.\nWe demonstrate the practical efficacy of DeepCell by applying it to critical\nEDA tasks such as functional Engineering Change Orders (ECO) and technology\nmapping. Extensive experimental results show that DeepCell significantly\nsurpasses state-of-the-art open-source EDA tools in efficiency and performance.",
    "pdf_url": "http://arxiv.org/pdf/2502.06816v2",
    "published": "2025-02-05"
  },
  "2102.10377v1": {
    "title": "CellTrack R-CNN: A Novel End-To-End Deep Neural Network for Cell Segmentation and Tracking in Microscopy Images",
    "authors": [
      "Yuqian Chen",
      "Yang Song",
      "Chaoyi Zhang",
      "Fan Zhang",
      "Lauren O'Donnell",
      "Wojciech Chrzanowski",
      "Weidong Cai"
    ],
    "summary": "Cell segmentation and tracking in microscopy images are of great significance\nto new discoveries in biology and medicine. In this study, we propose a novel\napproach to combine cell segmentation and cell tracking into a unified\nend-to-end deep learning based framework, where cell detection and segmentation\nare performed with a current instance segmentation pipeline and cell tracking\nis implemented by integrating Siamese Network with the pipeline. Besides,\ntracking performance is improved by incorporating spatial information into the\nnetwork and fusing spatial and visual prediction. Our approach was evaluated on\nthe DeepCell benchmark dataset. Despite being simple and efficient, our method\noutperforms state-of-the-art algorithms in terms of both cell segmentation and\ncell tracking accuracies.",
    "pdf_url": "http://arxiv.org/pdf/2102.10377v1",
    "published": "2021-02-20"
  },
  "2311.11004v1": {
    "title": "A Foundation Model for Cell Segmentation",
    "authors": [
      "Uriah Israel",
      "Markus Marks",
      "Rohit Dilip",
      "Qilin Li",
      "Morgan Schwartz",
      "Elora Pradhan",
      "Edward Pao",
      "Shenyi Li",
      "Alexander Pearson-Goulart",
      "Pietro Perona",
      "Georgia Gkioxari",
      "Ross Barnowski",
      "Yisong Yue",
      "David Van Valen"
    ],
    "summary": "Cells are the fundamental unit of biological organization, and identifying\nthem in imaging data - cell segmentation - is a critical task for various\ncellular imaging experiments. While deep learning methods have led to\nsubstantial progress on this problem, models that have seen wide use are\nspecialist models that work well for specific domains. Methods that have\nlearned the general notion of \"what is a cell\" and can identify them across\ndifferent domains of cellular imaging data have proven elusive. In this work,\nwe present CellSAM, a foundation model for cell segmentation that generalizes\nacross diverse cellular imaging data. CellSAM builds on top of the Segment\nAnything Model (SAM) by developing a prompt engineering approach to mask\ngeneration. We train an object detector, CellFinder, to automatically detect\ncells and prompt SAM to generate segmentations. We show that this approach\nallows a single model to achieve state-of-the-art performance for segmenting\nimages of mammalian cells (in tissues and cell culture), yeast, and bacteria\ncollected with various imaging modalities. To enable accessibility, we\nintegrate CellSAM into DeepCell Label to further accelerate human-in-the-loop\nlabeling strategies for cellular imaging data. A deployed version of CellSAM is\navailable at https://label-dev.deepcell.org/.",
    "pdf_url": "http://arxiv.org/pdf/2311.11004v1",
    "published": "2023-11-18"
  }
}